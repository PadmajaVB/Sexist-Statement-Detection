{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SexistCommentDetection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadmajaVB/Sexist-Statement-Detection/blob/main/SexistCommentDetection_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhjB3VYNVOSu"
      },
      "source": [
        "#!pip install tensorflow-addons"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0FphptPMkd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3089e4-2d55-46c7-9b5d-c2b52f40de9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA47SO9xmsjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7218b6-18b6-499c-b2cf-43887e752b6e"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhk9tctaQQpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff518221-d45b-4505-cea2-385ed06eab1e"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import collections\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential,model_from_json, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, SimpleRNN, Embedding, Dense, Flatten, RepeatVector, Permute, Activation, Dropout, Bidirectional, Multiply, Lambda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eO6WjrntR09"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nvc7OzPtNAB"
      },
      "source": [
        "## Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-yXKZVNgLDE"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/AI-3 Project/data/Preprocessed data/final_dataset.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feL7f9Jy-fS-"
      },
      "source": [
        "df_test = pd.read_excel('/content/drive/MyDrive/UnivAI/AI-3 Project/data/ISEP Sexist Data labeling.xlsx')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sweOhve-hFch"
      },
      "source": [
        "X = df[['Sentences']]\n",
        "y = df['Label']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1WcNSuiAN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6591d1e7-5dc9-4bc2-9935-43e20faf6122"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10512,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWFdvcm0-0Ja"
      },
      "source": [
        "X_test = df_test[['Sentences']]\n",
        "y_test = df_test['Label']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNBrmsPbtQXw"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, shuffle=True, random_state = 66, stratify=y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C86IXOZTm1OL"
      },
      "source": [
        "#!unzip '/content/drive/MyDrive/UnivAI/AI-3 Project/Embeddings/1b-GNGloVe-300d-0.8-0.8.zip' -d '/content/drive/MyDrive/UnivAI/AI-3 Project/Embeddings/'"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtlBSBMuEtf"
      },
      "source": [
        "### Loading the Gender neutral GLoVe embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZPM5CmhyOW"
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, encoding=\"utf8\") as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-XGDGMLi1s-"
      },
      "source": [
        "embedding_path = '/content/drive/MyDrive/UnivAI/AI-3 Project/Embeddings/1b-vectors300-0.8-0.8.txt'\n",
        "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs(embedding_path)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSb0Nv5HmzbU",
        "outputId": "d34acf71-aa5d-42bd-c8cd-0e9c53abb474"
      },
      "source": [
        "word_to_vec_map['and'].shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYEOZlatulo1"
      },
      "source": [
        "vocab = list(words_to_index.keys())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6cZK0YronTS",
        "outputId": "8468a709-5168-4976-b66d-bb25bf6b51d6"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbWhTQxvuMEz"
      },
      "source": [
        "### Getting word embeddigs for the input data - X_train and X_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzCUzLfPpArL"
      },
      "source": [
        "def get_tokens_embedding_list(data, vocab, word_to_vec_map):\n",
        "  embedding_list = []\n",
        "  for sent_seq in data:\n",
        "    tensor_list=[]\n",
        "    for word in sent_seq[0].split():\n",
        "      if word in vocab:\n",
        "        tensor_list.append(word_to_vec_map[word])\n",
        "        #print(len(tensor_list))\n",
        "    embedding_list.append(tensor_list)\n",
        "  return embedding_list"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C50-ku8LtBWD"
      },
      "source": [
        "X_train_embedding = get_tokens_embedding_list(X_train.values, vocab, word_to_vec_map)\n",
        "X_val_embedding = get_tokens_embedding_list(X_val.values, vocab, word_to_vec_map)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9lA1QTC-t2T"
      },
      "source": [
        "X_test_embedding = get_tokens_embedding_list(X_test.values, vocab, word_to_vec_map)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NY712TvSpQf"
      },
      "source": [
        "# X_train, X_val, y_train, y_val = train_test_split(X_train_embedding, y_train, train_size=0.9, random_state=66)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Z_7VUUm36j"
      },
      "source": [
        "def create_dataset(data_in, target_in):\n",
        "   \n",
        "    #Get the length of each sentence\n",
        "    N = [len(data_in[i]) for i, _ in enumerate(data_in)]\n",
        "    embedding_size = 300\n",
        "    tensor_N = tf.constant(N, tf.int32)\n",
        "    ragged_input  = tf.ragged.constant(data_in, dtype=tf.float32)\n",
        "    print(ragged_input.shape)\n",
        "\n",
        "    # Build the dataset and the operations\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((ragged_input, target_in))\n",
        "    del ragged_input\n",
        "\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(64)\n",
        "    \n",
        "    # Transform_pad function is defined above; you can change the num_parallel_calls\n",
        "    dataset = dataset.map(lambda x,y: (x.to_tensor(default_value=0, shape=[None, None, embedding_size]), y), num_parallel_calls=3)   \n",
        "                          \n",
        "    dataset = dataset.prefetch(1)\n",
        "    return dataset"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmEK4qIQSARA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03b3970-7eb4-422b-d2a0-22608032910d"
      },
      "source": [
        "train_dataset = create_dataset(X_train_embedding, y_train)\n",
        "val_dataset = create_dataset(X_val_embedding, y_val)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8409, None, None)\n",
            "(2103, None, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IZ-86rZT7oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b32f743-7e0d-4959-da82-43ce62aa2347"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, None, 300), (None,)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B62CItjH2wU5"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffRZ381EwVQo"
      },
      "source": [
        "input_layer = Input((None, 300), name='input')\n",
        "bid_lstm_1 = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "dropout_1 = Dropout(0.5)(bid_lstm_1)\n",
        "bid_lstm_2 = Bidirectional(LSTM(128, return_sequences=False))(dropout_1)\n",
        "dropout_2 = Dropout(0.5)(bid_lstm_2)\n",
        "dense = Dense(1, activation='sigmoid')(dropout_2)\n",
        "\n",
        "lstm_model = Model(inputs=input_layer, outputs=dense)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7he80iz31fr",
        "outputId": "1a889ac5-6316-4757-b1c0-d500f0837a69"
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None, 300)]       0         \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, None, 256)         439296    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 833,793\n",
            "Trainable params: 833,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbTGrKWC4pAY"
      },
      "source": [
        "lstm_model.compile(optimizer='adam', loss='bce', metrics=['accuracy'])"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gnOGWd05BRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de148348-bb34-45da-bc7d-57030620b09e"
      },
      "source": [
        "history = lstm_model.fit(train_dataset, epochs=10, validation_data=val_dataset, verbose=1)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "132/132 [==============================] - 31s 190ms/step - loss: 0.3807 - accuracy: 0.8320 - val_loss: 0.3539 - val_accuracy: 0.8445\n",
            "Epoch 2/10\n",
            "132/132 [==============================] - 24s 186ms/step - loss: 0.2769 - accuracy: 0.8871 - val_loss: 0.2828 - val_accuracy: 0.8787\n",
            "Epoch 3/10\n",
            "132/132 [==============================] - 24s 184ms/step - loss: 0.2444 - accuracy: 0.8998 - val_loss: 0.2916 - val_accuracy: 0.8849\n",
            "Epoch 4/10\n",
            "132/132 [==============================] - 24s 183ms/step - loss: 0.2157 - accuracy: 0.9116 - val_loss: 0.2668 - val_accuracy: 0.8887\n",
            "Epoch 5/10\n",
            "132/132 [==============================] - 24s 183ms/step - loss: 0.1960 - accuracy: 0.9202 - val_loss: 0.3015 - val_accuracy: 0.8906\n",
            "Epoch 6/10\n",
            "132/132 [==============================] - 24s 181ms/step - loss: 0.1615 - accuracy: 0.9388 - val_loss: 0.3053 - val_accuracy: 0.8864\n",
            "Epoch 7/10\n",
            "132/132 [==============================] - 24s 181ms/step - loss: 0.1393 - accuracy: 0.9457 - val_loss: 0.3610 - val_accuracy: 0.8721\n",
            "Epoch 8/10\n",
            "132/132 [==============================] - 24s 183ms/step - loss: 0.1172 - accuracy: 0.9556 - val_loss: 0.3358 - val_accuracy: 0.8773\n",
            "Epoch 9/10\n",
            "132/132 [==============================] - 24s 186ms/step - loss: 0.1054 - accuracy: 0.9622 - val_loss: 0.3436 - val_accuracy: 0.8783\n",
            "Epoch 10/10\n",
            "132/132 [==============================] - 24s 180ms/step - loss: 0.0930 - accuracy: 0.9646 - val_loss: 0.3240 - val_accuracy: 0.8954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjTdSKyKPZ9l",
        "outputId": "e8c53b5a-288c-4b5a-9d97-05ba163eb4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lstm_model.save('/content/drive/MyDrive/AI-3 Project/Models/lstm_model')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_37_layer_call_fn, lstm_cell_37_layer_call_and_return_conditional_losses, lstm_cell_38_layer_call_fn, lstm_cell_38_layer_call_and_return_conditional_losses, lstm_cell_40_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/AI-3 Project/Models/lstm_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/AI-3 Project/Models/lstm_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_xf_ccdSSrN"
      },
      "source": [
        "lstm_model = load_model('/content/drive/MyDrive/AI-3 Project/Models/lstm_model')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHQm7witwYuR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lin7eM2yrM8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GABMYB3Zk2C"
      },
      "source": [
        "ragged_input_test  = tf.ragged.constant(X_test_embedding, dtype=tf.float32)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((ragged_input_test, y_test.values))\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.batch(1)\n",
        "test_dataset = dataset.map(lambda x,y: (x.to_tensor(default_value=0, shape=[None, None, 300]),y), num_parallel_calls=3)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVhyuaFUTvSq"
      },
      "source": [
        "y_pred = lstm_model.predict(test_1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6jf2Xd6LUmT"
      },
      "source": [
        "## Model with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic7H3XtwLYYw"
      },
      "source": [
        "input_layer = Input((None, 300), name='input')\n",
        "bid_lstm_1 = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "dropout_1 = Dropout(0.5)(bid_lstm_1)\n",
        "\n",
        "attention = Dense(1, activation='tanh')(dropout_1)\n",
        "attention = Flatten()(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "attention = RepeatVector(128 * 2)(attention)\n",
        "attention = Permute([2, 1])(attention)\n",
        "\n",
        "sent_representation = Multiply()([dropout_1, attention])\n",
        "sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "\n",
        "dropout_2 = Dropout(0.5)(sent_representation)\n",
        "dense = Dense(1, activation='sigmoid')(dropout_2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=dense)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppDUmA6LLaUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b104895-c395-423d-febb-e0fbec2f056b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_9 (Bidirectional) (None, None, 256)    439296      input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, None, 256)    0           bidirectional_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, None, 1)      257         dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, None)         0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 256, None)    0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, None, 256)    0           repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, None, 256)    0           dropout_10[0][0]                 \n",
            "                                                                 permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 256)          0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            257         dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 439,810\n",
            "Trainable params: 439,810\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZm0UQyyPm0B"
      },
      "source": [
        "model.compile(optimizer='adam', loss='bce', metrics=['accuracy'])"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWf0IWEXPuPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bc78dd-2c1c-469d-ed66-808595e3dd07"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, verbose=1)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "132/132 [==============================] - 18s 117ms/step - loss: 0.4868 - accuracy: 0.7623 - val_loss: 0.4566 - val_accuracy: 0.7499\n",
            "Epoch 2/10\n",
            "132/132 [==============================] - 16s 118ms/step - loss: 0.4607 - accuracy: 0.7946 - val_loss: 0.4885 - val_accuracy: 0.7760\n",
            "Epoch 3/10\n",
            "132/132 [==============================] - 16s 118ms/step - loss: 0.3941 - accuracy: 0.8352 - val_loss: 0.3813 - val_accuracy: 0.8326\n",
            "Epoch 4/10\n",
            "132/132 [==============================] - 15s 114ms/step - loss: 0.3300 - accuracy: 0.8606 - val_loss: 0.3790 - val_accuracy: 0.8250\n",
            "Epoch 5/10\n",
            "132/132 [==============================] - 15s 113ms/step - loss: 0.3157 - accuracy: 0.8662 - val_loss: 0.3625 - val_accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "132/132 [==============================] - 15s 117ms/step - loss: 0.2991 - accuracy: 0.8735 - val_loss: 0.3728 - val_accuracy: 0.8298\n",
            "Epoch 7/10\n",
            "132/132 [==============================] - 16s 118ms/step - loss: 0.2992 - accuracy: 0.8757 - val_loss: 0.3424 - val_accuracy: 0.8550\n",
            "Epoch 8/10\n",
            "132/132 [==============================] - 15s 113ms/step - loss: 0.2825 - accuracy: 0.8868 - val_loss: 0.3276 - val_accuracy: 0.8559\n",
            "Epoch 9/10\n",
            "132/132 [==============================] - 16s 121ms/step - loss: 0.2623 - accuracy: 0.8969 - val_loss: 0.3280 - val_accuracy: 0.8545\n",
            "Epoch 10/10\n",
            "132/132 [==============================] - 15s 113ms/step - loss: 0.2582 - accuracy: 0.8949 - val_loss: 0.3464 - val_accuracy: 0.8645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ9eoncOO8RZ",
        "outputId": "925ddc60-e91e-4c70-8f48-acfec1c8e618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save('/content/drive/MyDrive/AI-3 Project/Models/lstm_with_attention')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_28_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn, lstm_cell_29_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/AI-3 Project/Models/lstm_with_attention/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/AI-3 Project/Models/lstm_with_attention/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59_TuW42KylM"
      },
      "source": [
        "### Script to get the data for everyday Sexism project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4D1R9JiNWiK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "cc60b56d-73e7-4b35-c722-8747e7d9e94c"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/43/eb2be141dac56e502b6e35c1e4a9b1bbb2d4dcbec773c0f6563e79758909/ipdb-0.13.8.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (56.1.0)\n",
            "Collecting ipython>=7.17.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d1/8d0ba7589ea4cbf3e80ef8e20616da2cfc3c33187a64b044372aad517512/ipython-7.23.1-py3-none-any.whl (785kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e6/4b4ca4fa94462d4560ba2f4e62e62108ab07be2e16a92e594e43b12d3300/prompt_toolkit-3.0.18-py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.0.5)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.8-cp37-none-any.whl size=11599 sha256=d839fd73980ff3fe86fe9f9ed5af4b3dc5f8d906d28d4410b1267be7ab091db3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/d6/5a/2fdf30b75ca5099e18f66a0a4d439ba031e1aa239e12b39c24\n",
            "Successfully built ipdb\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.23.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed ipdb-0.13.8 ipython-7.23.1 prompt-toolkit-3.0.18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2D3qViJNq0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5bf105-c6ae-4b4e-b1d2-178d3cdd927b"
      },
      "source": [
        "!apt-get install libmagic-dev\n",
        "!pip install python-magic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "Suggested packages:\n",
            "  file\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-dev libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 332 kB of archives.\n",
            "After this operation, 5,552 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-dev amd64 1:5.32-2ubuntu0.4 [79.7 kB]\n",
            "Fetched 332 kB in 1s (227 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic-dev:amd64.\n",
            "Preparing to unpack .../libmagic-dev_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-dev:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-dev:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting python-magic\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/7c/1d1d4bdda29bfec662b9b50951dee2dddf7747d3cbf7777f3d1c63372bd0/python_magic-0.4.22-py2.py3-none-any.whl\n",
            "Installing collected packages: python-magic\n",
            "Successfully installed python-magic-0.4.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tzpYpiKv4sB",
        "outputId": "98a5d504-dc63-4b22-e6ed-4575a8d4aa3c"
      },
      "source": [
        "!pip install ftfy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41916 sha256=716904f52b70aaf841cf383a11fe4f203a4e08021cda22b0b21c2ad60ba7f1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6YVmAjEQC1r"
      },
      "source": [
        "# Example use: python script.py data_placeholders.tsv data.tsv\n",
        "import re\n",
        "import urllib.request as urllib2\n",
        "import os\n",
        "import ipdb\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import sys\n",
        "import codecs\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import io\n",
        "import magic\n",
        "import pickle\n",
        "import ftfy\n",
        "\n",
        "tsv_filepath = '/content/drive/MyDrive/UnivAI/AI-3 Project/data/Everyday Sexism Project/data_placeholders.tsv'\n",
        "data_write_path = '/content/drive/MyDrive/UnivAI/AI-3 Project/data/Everyday Sexism Project/data.tsv'\n",
        "\n",
        "\n",
        "def find_post_id(name_box):\n",
        "    classes = name_box['class']\n",
        "    for _class in classes:\n",
        "        if _class[:5] == \"post-\" and len(_class) > 5:\n",
        "            return int(_class[5:])\n",
        "    return -1\n",
        "\n",
        "\n",
        "def crawl_post(post_page, post_number):\n",
        "    crawl_page = post_page + str(post_number)\n",
        "\n",
        "    isCrawledSuccessfully = False\n",
        "    while not isCrawledSuccessfully:\n",
        "        try:\n",
        "            page = urllib2.urlopen(crawl_page).read()\n",
        "            isCrawledSuccessfully = True\n",
        "        except urllib2.HTTPError as e:\n",
        "            if e.code == 404:\n",
        "                print(\"\\t\\t>>> Found 404\")\n",
        "                return None\n",
        "            else:\n",
        "                print(\"\\t\\t>>> Waitiing for 30 seconds\")\n",
        "                time.sleep(30)\n",
        "        except Exception:\n",
        "            print(\"\\t\\t>>> Waitiing for 30 seconds\")\n",
        "            time.sleep(30)\n",
        "\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    name_boxes = soup.find_all('article', attrs={'class': 'post'})\n",
        "    assert len(name_boxes) == 1\n",
        "    for i, name_box in enumerate(name_boxes):\n",
        "        post_id = find_post_id(name_box)\n",
        "        if post_id == -1:\n",
        "            print(\"\\t\\t>>> ERROR: Error fetching post_id\")\n",
        "            return None\n",
        "        \n",
        "        #TODO: VERIFY IF THIS IS CORRECT\n",
        "        soup = BeautifulSoup(name_box.encode(\"utf-8\"), 'html.parser')\n",
        "        _name_box = soup.find('div', attrs={'class': 'entry-content'})\n",
        "        post = _name_box.get_text()\n",
        "        post = post.replace('\\n', '<br/>')\n",
        "    return post\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "r_white = re.compile(r'\\s+')\n",
        "\n",
        "xml_csv = open(data_write_path, 'w')\n",
        "csv_writer = csv.writer(xml_csv, delimiter='\\t')\n",
        "csv_writer.writerow(['post', 'labels'])\n",
        "\n",
        "post_id_dict = {}\n",
        "\n",
        "with open(tsv_filepath, 'r') as csvfile:\n",
        "    spamreader = csv.reader(csvfile, delimiter='\\t')\n",
        "    for i, row in enumerate(spamreader):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        labels = row[1]\n",
        "        parts = row[0].split(\"__\")\n",
        "        post_id__str = parts[0]\n",
        "        post_offsets__str = [offset.split(\"_\") for offset in parts[1:]]\n",
        "\n",
        "        if post_id__str in post_id_dict:\n",
        "            print(\"accessing \" + post_id__str)\n",
        "            post_text = post_id_dict[post_id__str]\n",
        "        else:\n",
        "            print(\"fetching \" + post_id__str)\n",
        "            post_text = crawl_post(\"https://everydaysexism.com/everyday-sexism/\", post_id__str)\n",
        "            if post_text == None:\n",
        "                continue\n",
        "            post_id_dict[post_id__str] = post_text\n",
        "\n",
        "        post_text = post_text.replace('<br/>', ' ')\n",
        "        post_text = re.sub(' +', ' ', post_text)\n",
        "        post_text = post_text.lstrip()\n",
        "        post_text__fixed = ftfy.fix_text(post_text)\n",
        "        text = \"\"\n",
        "        for index in range(len(post_offsets__str)):\n",
        "            text = text + post_text__fixed[int(post_offsets__str[index][0]):int(post_offsets__str[index][1]) + 1]\n",
        "            if len(post_offsets__str) > 1 and len(post_offsets__str) != index + 1:\n",
        "                text = text + \" \"\n",
        "        text = r_white.sub(' ', text)\n",
        "        text = text.strip()\n",
        "        csv_writer.writerow([text, labels])\n",
        "\n",
        "xml_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mdpyM9LQEhS"
      },
      "source": [
        "df_es = pd.read_csv(data_write_path, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}